---
title: "Forecasting Inflation"
subtitle: "Comparing models"
author: "Duncan Mugford"
date: "`r format(Sys.Date(),'%B %d, %Y')`"
output:
  html_document:
    df_print: paged
    code_folding: "hide"
    toc: yes
    fig_caption: yes
    theme: cerulean
    toc_float: no
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
graphics.off()
```
Project available under projects on: [My Github Page](https://DuncanJMUG.github.io)

Direct link to project: [Project](https://duncanjmug.github.io/Project.html)

```{r load packages, message= FALSE, warning=FALSE}
require(mdsr)
require(tidyverse)
require(ggplot2)
require(fpp3)
require(tidyquant)
require(slider)
require(tsibble)
require(stargazer)
require(kableExtra)
require(fma)
require(fable)
require(reshape2)

```

The first step in attempting to forecast inflation is correctly defining our data. Data has been downloaded from FRED, with the variables singled out being TCU,MICH,UNRATE,PCEPI, and IPMAN.
We gathered these variables in specific as there is reason to believe they could hold an impact on forecasting inflation. 

>Defining the variables

PCEPI is Personal Consumption Expenditures: Chain-type Price Index. This is a variable of interest as inflation can commonly be looked at as the changes in price over time of certain bundles of goods purchased by groups.

UNRATE is Unemployment Rate. This is important because when using a philips curve, there is a distinct relation between unemployment rate and inflation:
\begin{align}
\pi = \phi + \pi + Y * \mu 
\end{align}
The Philips Curve therefore shows us that UNRATE has a direct relationship with inflation, thus why we include it as a variable of interest.

IPMAN is manufacturing industrial production seasonally adjusted and using 2017 as the index (2017=100). IPMAN is important for the purposes of inflation as it can help us to examine the possible relationship between manufacturing and inflation. We would expect it to trail inflation, with higher inflation leading to lower industrial production.

MICH is the University of Michigan's inflation expectation which is important expectations of the population are generally found to have a leading effect due to the changes in behavior that stem from shifts in expectations actually leading into causing expected outcomes.

Finally, TCU is the Total Index of the Capacity Utilization, so how much of the total production capacity is being utilized in a given time period. This should have a relationship with inflation in a similar manner to IPMAN, as inflation rises and leads into a recession total capacity utilization will likely fall, this likely follows inflation but if it is the other way around it would only make TCU more useful as a variable.
```{r load data, MESSAGES = FALSE}
FREDdata <- read.csv("C:\\Users\\chumb\\Documents\\R\\fred.csv")

LBSSA12 <- read.csv("C:\\Users\\chumb\\Documents\\R\\LBSSA12.csv")

varList2 <-
  c(
    "PCEPI",
    "UNRATE",
    "MICH",
    "IPMAN",
    "TCU"
  )
X <- tq_get(varList2, get = "economic.data", from = "1982-12-01") %>%
  mutate(Month = yearmonth(date)) %>% dplyr::select(-date) %>%
  as_tsibble(index = Month, key = symbol)
Xw <- X %>%
  pivot_wider(names_from = symbol, values_from = price) %>%
  as_tsibble()


```


```{r Zsetup transform}

Z <- Xw %>% select(c(PCEPI, UNRATE, MICH, TCU, IPMAN)) %>%
  mutate(infl = 1200*log(PCEPI/lag(PCEPI))) %>% 
  mutate(dinfl = infl - lag(infl,1)) %>% 
  mutate(dinfl12 = 100*log(PCEPI/lag(PCEPI,12)) - lag(infl,12)) %>% 
  mutate(unrate = UNRATE - lag(UNRATE)) %>% 
  mutate(tcu = TCU - lag(TCU)) %>% 
  mutate(ipman = 1200*log(IPMAN/lag(IPMAN))) %>% 
  mutate(mich  = 10*log(MICH/lag(MICH))) %>% 
  select(-c(PCEPI, UNRATE, TCU, MICH, IPMAN)) %>% 
  drop_na()

train_data <- Z %>% filter_index(~ "2018-12")
test_data <- Z %>% filter_index("2019-01" ~ .)
```
>Transform the data

Pictured below are our variables transformed into stationary variables so that we can use them in our model. They all seem to behave appropriately in the context of each variable means. TCU and UNRATE have close relations to inflation, outside of 2020 where unemployment and total capacity received a drastic shock due to coronavirus. IPMAN has a similar trend to TCU, as expected. MICH has an outlier around 2002 but outside of that seems to follow inflation fairly well. 
```{R MELT}
Zm <- melt(Z, "Month")
ggplot(Zm, aes(Month, value)) + 
  geom_line() + 
  facet_wrap(~variable, scales = "free", ncol = 2)
```

>The Philips Curve Model

Here we form our Philips Curve, using 12 lags on dinfl and unrate, as inflation is being based off of the relationship of lagged unemployment rate and lagged difference in inflation. 
Our F-statistic has an extremely low p-value showing that some of the variables used are explaining the patterns in inflation. 

```{r m1}
fitPC <- train_data %>% 
  model(
    mPC = TSLM(dinfl12 ~ 1 +
                 lag(dinfl,12) + lag(dinfl,13) + lag(dinfl,14) +
                 lag(dinfl,15) + lag(dinfl,16) + lag(dinfl,17) +
                 lag(dinfl,18) + lag(dinfl,19) + lag(dinfl,20) +
                 lag(dinfl,21) + lag(dinfl,22) + lag(dinfl,23) +
                 lag(unrate,12) + lag(unrate,13) + lag(unrate,14) +
                 lag(unrate,15) + lag(unrate,16) + lag(unrate,17) +
                 lag(unrate,18) + lag(unrate,19) + lag(unrate,20) +
                 lag(unrate,21) + lag(unrate,22) + lag(unrate,23) 
                 )
  )
report(fitPC)
#gg_tsresiduals(fitPC)
```
>Models on the other variables

Next we redefine our model using the TCU/MICH/IPMAN variables in place of UNRATE for 3 new models. By adding all 3 of these models to the same table as the UNRATE philips curve model, we can more easily compare them.

```{r m2/3/4}


fitPC2 <- train_data %>% 
  model(
    mPC1 = TSLM(dinfl12 ~ 1 +
                 lag(dinfl,12) + lag(dinfl,13) + lag(dinfl,14) +
                 lag(dinfl,15) + lag(dinfl,16) + lag(dinfl,17) +
                 lag(dinfl,18) + lag(dinfl,19) + lag(dinfl,20) +
                 lag(dinfl,21) + lag(dinfl,22) + lag(dinfl,23) +
                 lag(unrate,12) + lag(unrate,13) + lag(unrate,14) +
                 lag(unrate,15) + lag(unrate,16) + lag(unrate,17) +
                 lag(unrate,18) + lag(unrate,19) + lag(unrate,20) +
                 lag(unrate,21) + lag(unrate,22) + lag(unrate,23) 
                 )
  )

fitall <- train_data %>% 
  model(
      mUR = TSLM(dinfl12 ~ 1 +
                 lag(dinfl,12) + lag(dinfl,13) + lag(dinfl,14) +
                 lag(dinfl,15) + lag(dinfl,16) + lag(dinfl,17) +
                 lag(dinfl,18) + lag(dinfl,19) + lag(dinfl,20) +
                 lag(dinfl,21) + lag(dinfl,22) + lag(dinfl,23) +
                 lag(unrate,12) + lag(unrate,13) + lag(unrate,14) +
                 lag(unrate,15) + lag(unrate,16) + lag(unrate,17) +
                 lag(unrate,18) + lag(unrate,19) + lag(unrate,20) +
                 lag(unrate,21) + lag(unrate,22) + lag(unrate,23) 
                 ),
    mIPMAN = TSLM(dinfl12 ~ 1 +
                 lag(dinfl,12) + lag(dinfl,13) + lag(dinfl,14) +
                 lag(dinfl,15) + lag(dinfl,16) + lag(dinfl,17) +
                 lag(dinfl,18) + lag(dinfl,19) + lag(dinfl,20) +
                 lag(dinfl,21) + lag(dinfl,22) + lag(dinfl,23) +
                 lag(ipman,12) + lag(ipman,13) + lag(ipman,14) +
                 lag(ipman,15) + lag(ipman,16) + lag(ipman,17) +
                 lag(ipman,18) + lag(ipman,19) + lag(ipman,20) +
                 lag(ipman,21) + lag(ipman,22) + lag(ipman,23) 
    ),
     mMICH = TSLM(dinfl12 ~ 1 +
                 lag(dinfl,12) + lag(dinfl,13) + lag(dinfl,14) +
                 lag(dinfl,15) + lag(dinfl,16) + lag(dinfl,17) +
                 lag(dinfl,18) + lag(dinfl,19) + lag(dinfl,20) +
                 lag(dinfl,21) + lag(dinfl,22) + lag(dinfl,23) +
                 lag(mich,12) + lag(mich,13) + lag(mich,14) +
                 lag(mich,15) + lag(mich,16) + lag(mich,17) +
                 lag(mich,18) + lag(mich,19) + lag(mich,20) +
                 lag(mich,21) + lag(mich,22) + lag(mich,23) 
                 ),
    mTCU = TSLM(dinfl12 ~ 1 +
                 lag(dinfl,12) + lag(dinfl,13) + lag(dinfl,14) +
                 lag(dinfl,15) + lag(dinfl,16) + lag(dinfl,17) +
                 lag(dinfl,18) + lag(dinfl,19) + lag(dinfl,20) +
                 lag(dinfl,21) + lag(dinfl,22) + lag(dinfl,23) +
                 lag(tcu,12) + lag(tcu,13) + lag(tcu,14) +
                 lag(tcu,15) + lag(tcu,16) + lag(tcu,17) +
                 lag(tcu,18) + lag(tcu,19) + lag(tcu,20) +
                 lag(tcu,21) + lag(tcu,22) + lag(tcu,23) 
                 )
  )


tidy(fitall)

```
>Accuracy of the models compared

Out of the four models we are comparing, the MICH variable for people's expectations of inflation has the lowest Root Mean Squared Error, Mean Absolute Error, and Mean Absolute Standard Error. Overall MICH may perform better as a predictor than our other variables presented possibly due to how intertwined expectations of inflation and real inflation rates actually are.

```{r accuracy check}
accuracy(fitall)
```

>Residuals of MICH

However, when we select the MICH model on its own to observe its residuals we can quickly see that the model may not be a great fit for our data. There are a large amount of residuals shown to be outside of the normally accepted "white noise" range.

```{r MICH residuals,warning= FALSE}
fitall %>% select(mMICH) %>% gg_tsresiduals()
```

>Aggregate Model

Our next step is to make a model that aggregates all four of our previous models in an attempt to get a better fitting model for the data.

A quick check on the accuracy of our new combination model shows us that it has lower errors than MICH did on every accuracy measure.

```{r combomodel}
fit_combo <- fitall %>% mutate(combo = (mUR + mMICH + mIPMAN + mTCU)/4)

accuracy(fit_combo)
```

>Forecast of the 5 Models

```{r graph models}


fc_combo <- fit_combo %>% forecast(new_data = test_data)
fc_combo %>% autoplot(filter(Z, year(Month) > 2016), level = c(95))

accuracy(fc_combo,test_data)
```

>Accuracy of the Forecast

The table above shows the accuracy of the four variable models as well as the accuracy of our combination model. Now, mTCU has the lowest Mean Absolute Percentage Error, the combo model has the lowest Mean Absolute Error, mMICH has the lowest Root Mean Standard Error, and mUR has the lowest Mean Error. Once we used our models in attempt to forecast our combination model quickly lost a lot of its relevancy. However, it still may be the best model out of the 5 because even on RMSE and MAPE it only trails behind slightly as being the less accurate model. 

>Conclusion

There are several things of note that came out of these forecasting exercises. First, is that the combination model defined on past data became less accurate as time went on for forecasting future inflation. This may seem intuitive, but shows us that it is important not to become too reliant on a single model and constantly be reassessing and judging the models we use for forecasting.

A second thing of note is that we weighted our mUR,mTCU,mIPMAN, and mMICH models all the same when defining our combination model on these four variables. In reality, this may not be the case. TCU and IPMAN were shown to be closely related when transformed so it may be better to weight one of them less in case they have a collinearity issue. Furthermore, based on the MICH models performance when we just tested the four individual variable models and the intuitive nature of expectations leading reality it may be useful in the future to give more weight to the mMICH model when defining a combination model.

A third point of consideration is the models seen here are based only off a handful of possible variables. In reality it may be better to test a larger amount of variables from the FRED data and determine which ones have the largest accuracy in determining inflation as well as low amount of collinearity between each other and use those to form a combination model.